\documentclass[mathserif]{beamer}
%\documentclass[mathserif,handout]{beamer}

%\usepackage{algorithm2e}
\usepackage{mathtools}
\usepackage{booktabs}
\usetheme{metropolis}
%\mode{presentation}

\DeclareMathOperator{\struckDelta}{\Delta \left( \mathbf{y}, \mathbf{y}_i \right)}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\st}{\text{s.t.} \quad}

% front matter
\title{Struck}
\subtitle{Structured Output Tracking with Kernels}
\author{Brendan Robeson}
\date[CS 7680]{CS 7680 - Advanced Computer Vision}
\institute{Utah State University}

\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\input{tracking_by_detection}

\section{Struck}

\begin{frame}
    \frametitle{Here's the gist...}

    \begin{itemize}
        \item Don't learn a binary classifier. Learn a prediction function: $f :
            \mathcal{X} \rightarrow \mathcal{Y}$.
        \item Labelled sample is $(\mathbf{x}, \mathbf{y})$.
        \item $f$ is learned in a structured output SVM.
        \item Use a discriminant function $F: \mathcal{X} \times \mathcal{Y}
            \rightarrow \mathbb{R}$.
        \item Prediction function becomes %\uncover<1>{
            \begin{align}
                \mathbf{y}_t &= \argmax\limits_{y \in \mathcal{Y}} h \left( \mathbf{x}_t^{\mathbf{p}_{t-1}}, \mathbf{y} \right) \tag{prior} \\
                \mathbf{y}_t &= \argmax\limits_{y \in \mathcal{Y}} F \left( \mathbf{x}_t^{\mathbf{p}_{t-1}}, \mathbf{y} \right) \tag{Struck}
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{What is the discriminant function, $F$?}
    \begin{equation}
        F(\mathbf{x}, \mathbf{y}) = \langle \mathbf{w}, \mathbf{\Phi}(\mathbf{x}, \mathbf{y}\rangle
    \end{equation}
    \begin{equation} \label{eq:minimizing_hyperplane}
        \begin{aligned}
            \min_{\mathbf{w}} \quad & \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \\
            \st & \forall i : \xi \ge 0 \\
                & \forall i, \forall \mathbf{y} \ne \mathbf{y}_i : \langle \mathbf{w}, \delta \mathbf{\Phi}_i (\mathbf{y}) \rangle \ge \Delta (\mathbf{y}_i, \mathbf{y}) - \xi_i
        \end{aligned}
    \end{equation}

    \begin{equation}
        \delta \mathbf{\Phi}_i (\mathbf{y}) = \mathbf{\Phi}(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{\Phi}(\mathbf{x}_i, \mathbf{y})
    \end{equation}
\end{frame}

\begin{frame}
    \frametitle{What is the discriminant function, $F$}
    \begin{itemize}
        \item $\Delta$ is a loss function.
            \begin{itemize}
                \item $\Delta (\mathbf{y}, \mathbf{\bar{y}}) = 0 \; \text{iff} \; \mathbf{y} = \mathbf{\bar{y}}$
                \item $\Delta$ should decrease to 0 as $\mathbf{y}$ approaches $\mathbf{\bar{y}}$
            \end{itemize}
        \item This overcomes the equal treatment of samples.
        \item The authors use a variation of bounding box overlap:
    \end{itemize}
    \begin{equation}
        \Delta(\mathbf{y}, \mathbf{\bar{y}}) = 1 - s_{\mathbf{p}_t}^o (\mathbf{y}, \mathbf{\bar{y}})
    \end{equation}
    \alert{What does this mean for $\Delta$?}
\end{frame}

\begin{frame}
    \frametitle{Optimizing equation \eqref{eq:minimizing_hyperplane}}
    \begin{equation}
        \min_{\mathbf{w}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \tag{\ref{eq:minimizing_hyperplane}}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \max_{\alpha} \quad & \sum_{i, \mathbf{y} \ne \mathbf{y}_i} \struckDelta \alpha_i^\mathbf{y} -
                \frac{1}{2} \sum_{\substack{i, \mathbf{y} \ne \mathbf{y}_i \\ j, \mathbf{y} \ne \mathbf{y}_j}}
                \alpha_i^\mathbf{y} \alpha_j^\mathbf{\bar{y}} \langle \delta \mathbf{\Phi}_i \left( \mathbf{\bar{y}} \right),
                \delta \mathbf{\Phi}_j \left( \mathbf{\bar{y}} \right) \rangle \\
            \st & \forall i, \forall \mathbf{y} \ne \mathbf{y}_i : \alpha_i^\mathbf{y} \ge 0 \\
                & \forall i \sum_{\mathbf{y} \ne \mathbf{y}_i} \alpha_i^\mathbf{y} \le C
        \end{aligned}
    \end{equation}
\end{frame}

\begin{frame}
    \frametitle{Optimizing equation \eqref{eq:minimizing_hyperplane}}
    Reparameterising based on ...
    \begin{equation}
        \beta_i^\mathbf{y} = \begin{dcases}
                                -\alpha_i^\mathbf{y} & \mathbf{y} \ne \mathbf{y}_i \\
                                \sum_{\mathbf{\bar{y}} \ne \mathbf{y}_i} \alpha_i^\mathbf{\bar{y}} & \mathbf{y} = \mathbf{y}_i
                             \end{dcases}
    \end{equation}

    \begin{equation}
        \begin{aligned}
            \max_{\beta} & -\sum_{i,\mathbf{y}} \struckDelta \beta_i^\mathbf{y} -
                \frac{1}{2} \sum_{i,\mathbf{y},j,\mathbf{\bar{y}}} \beta_i^\mathbf{y}
                \beta_j^\mathbf{\bar{y}} \langle \mathbf{\Phi}\left( \mathbf{x}_i, \mathbf{y}
                \right), \mathbf{\Phi}\left( \mathbf{x}_j, \mathbf{\bar{y}} \right) \rangle \\
            \st & \forall i, \forall \mathbf{y} : \beta_i^\mathbf{y} \le \delta(\mathbf{y}, \mathbf{y}_i) C \\
                & \forall i: \sum_\mathbf{y} \beta_i^\mathbf{y} = 0
        \end{aligned}
    \end{equation}
\end{frame}
\begin{frame}
    \frametitle{Optimizing equation \eqref{eq:minimizing_hyperplane}}
    \begin{gather}
        \delta(\mathbf{y}, \mathbf{\bar{y}}) = \begin{cases}
                                                    1 & \mathbf{y} = \mathbf{\bar{y}} \\
                                                    0 & \mathbf{y} \ne \mathbf{\bar{y}}
                                               \end{cases} \\
        F(\mathbf{x}, \mathbf{y}) = \sum_{i, \mathbf{\bar{y}}} \beta_i^\mathbf{\bar{y}} \langle
            \mathbf{\Phi}(\mathbf{x}_i, \mathbf{\bar{y}}), \mathbf{\Phi}(\mathbf{x}, \mathbf{y}) \rangle
    \end{gather}
\end{frame}

%\begin{frame}
%    \frametitle{How does Struck work?}
%    \begin{itemize}
%        \item Learning and tracking are integrated.
%        \item Uses a structured output SVM.
%        \item Must overcome the \alert{curse of kernelization.}
%            \begin{itemize}
%                \item Number of support vectors increase as a function of training data
%            \end{itemize}
%    \end{itemize}
%\end{frame}

\input{experiments}
\input{conclusion}

\end{document}

