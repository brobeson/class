\documentclass[mathserif,handout]{beamer}
%\documentclass{beamer}

%\usepackage{algorithm2e}
\usepackage{mathtools}
\usepackage{booktabs}
\usetheme{metropolis}
%\mode{presentation}

\DeclareMathOperator{\struckDelta}{\Delta \left( \mathbf{y}, \mathbf{y}_i \right)}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\st}{\text{s.t.} \quad}

% front matter
\title{Struck}
\subtitle{Structured Output Tracking with Kernels}
\author{Brendan Robeson}
\date[CS 7680]{CS 7680 - Advanced Computer Vision}
\institute{Utah State University}

\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{The Problem}

\begin{frame}
    \frametitle{Tracking can be treated as an object detection problem.}
    \begin{description}
        \item [Tracking by detection] Object detection in each frame.
        \item [Adaptive tracking by detection] Update the classifier online.
    \end{description}

    Separates sample generation from classifier updating.
\end{frame}

%\begin{frame}
%    \frametitle{So, what's wrong with the previous algorithms?}
%    \begin{enumerate}
%        \item<1-> How should samples be generated and labeled?
%
%            \uncover<2->{\alert{Most consider samples far from the target to be negative.}}
%        \item<1-> Classifier wants to determine \emph{what} an object is.
%
%            Tracker wants to determine \emph{where} an object is.
%
%            \uncover<3->{\alert{The two are not trained together. Thus, assuming highest classifier
%            output $\Rightarrow$ location is not always true.}}
%    \end{enumerate}
%    \uncover<4->{Most trackers try to make the classifier more robust with respect to poor samples.}
%\end{frame}

%\begin{frame}
%    \frametitle{Adaptive tracking-by-detection algorithm}
%    \begin{algorithm}[H]
%        \DontPrintSemicolon
%        \KwIn{Video frame: $f_t$}
%            \ForEach{ROI image}
%            {
%                Calculate the GLCM $P_u$\;
%                Calculate energy using Equation (2)\;
%                Calculate contrast using Equation (3)\;
%                Calculate correlation using Equation (4)\;
%                Calculate entropy using Equation (5)\;
%                Calculate homogeneity using Equation (6)\;
%            }
%            \KwOut{A set of feature vectors}
%    \end{algorithm}
%\end{frame}

\begin{frame}
    \frametitle{How does one train adaptive tracking-by-detection?}

    \begin{itemize}
        \item The algorithm operates on frame $f_t$, for $t \in \{1, 2, ..., T\}$.
        \item The tracker estimates a bounding box position, $\mathbf{p}$.
        \item Extract features $\mathbf{x}_t^\mathbf{p}$ from patches within the bounding box.
        \item Train the classifier with $(\mathbf{x}, z)$\uncover<3->{, where $z = \pm1$.}
    \end{itemize}

    \uncover<2>{\alert{What is $z$?}}
\end{frame}

\begin{frame}
    \frametitle{How does one predict using adaptive tracking-by-detection?}

    \begin{itemize}
        \item The goal is to estimate a transformation $\mathbf{y}_t \in
            \mathcal{Y}$, where $\mathbf{p}_t = \mathbf{p}_{t-1} \circ \mathbf{y}_t$.
        \item $\mathcal{Y}$ is the search space in the image.
        \item Typically, $\mathcal{Y} = \left\{ (u,v) | u^2 + v^2 < r^2 \right \}$
            \alert{$\Leftarrow$ What does this mean?}
        \item $\mathbf{y}_t = \argmax\limits_{y \in \mathcal{Y}} h \left(
            \mathbf{x}_t^{\mathbf{p}_{t-1}}, \mathbf{y} \right)$
        \item Uses a classification confidence function $h(\mathbf{x})$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How does one update the classifier?}

    \begin{itemize}
        \item Update the classifier after calculating $\mathbf{p}_t$.
        \item Generate sample transformations: $\left\{ \mathbf{y}_t^1,
            \mathbf{y}_t^2, ..., \mathbf{y}_t^n \right\}$.
        \item Generate training samples: $\left\{
                \mathbf{x}_t^{\mathbf{p}_t \circ \mathbf{y}_t^1},
                \mathbf{x}_t^{\mathbf{p}_t \circ \mathbf{y}_t^2}, ...,
                \mathbf{x}_t^{\mathbf{p}_t \circ \mathbf{y}_t^n} \right\}$.
        \item Generate labels: $\left\{ z_t^1, z_t^2, ..., z_t^n \right\}$.
        \item Use the samples and labels to update the classifier.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How to label a transformation.}
    Use a transformation similarity function:

    \begin{equation}
        s_{\mathbf{t}_t}^o \left( \mathbf{y}_t^i, \mathbf{y}_t^j \right) =
            \frac{\left( \mathbf{p}_t \circ \mathbf{y}_t^i \right) \cap \left( \mathbf{p}_t \circ \mathbf{y}_t^j \right)}
                 {\left( \mathbf{p}_t \circ \mathbf{y}_t^i \right) \cup \left( \mathbf{p}_t \circ \mathbf{y}_t^j \right)}
    \end{equation}

    Then, use a labelling function:

    \begin{equation}
        l\left( s_{\mathbf{p}_t}\left(\mathbf{y}^0, \mathbf{y}_t^i \right) \right) =
            \begin{cases}
                +1 & s_{\mathbf{p}_t}\left(\mathbf{y}^0, \mathbf{y}_t^i \right) > \theta_u \\
                -1 & s_{\mathbf{p}_t}\left(\mathbf{y}^0, \mathbf{y}_t^i \right) < \theta_l \\
                 0 & \text{otherwise}
            \end{cases}
    \end{equation}

    $\mathbf{y}^0$ is the identity transformation.
\end{frame}

\begin{frame}
    \frametitle{So what's wrong with this?}

    \begin{itemize}
        \item $(\mathbf{x}, z)$ does not correlate $\mathbf{y}$ with $z$.
        \item Training samples all have equal weight in the classifier.
            \begin{itemize}
                \item A background sample with significant overlap: $z = -1$.
                \item A background sample with less overlap: $z = -1$.
                \item This leads to tracking error growing over time.
            \end{itemize}
        \item The labeller, and the learner, are distinct.
            \begin{itemize}
                \item Errors by the labeller are noise in the learner.
                \item Current techniques attempt to make the learner more
                    robust, but not address the cause of the noise.
            \end{itemize}
    \end{itemize}
\end{frame}

%\section{Prior Solutions}

\section{Struck}

\begin{frame}
    \frametitle{Here's the gist...}

    \begin{itemize}
        \item Don't learn a binary classifier. Learn a prediction function: $f :
            \mathcal{X} \rightarrow \mathcal{Y}$.
        \item Labelled sample is $(\mathbf{x}, \mathbf{y})$.
        \item $f$ is learned in a structured output SVM.
        \item Use a discriminant function $F: \mathcal{X} \times \mathcal{Y}
            \rightarrow \mathbb{R}$.
        \item Prediction function becomes %\uncover<1>{
            \begin{align}
                \mathbf{y}_t &= \argmax\limits_{y \in \mathcal{Y}} h \left( \mathbf{x}_t^{\mathbf{p}_{t-1}}, \mathbf{y} \right) \tag{prior} \\
                \mathbf{y}_t &= \argmax\limits_{y \in \mathcal{Y}} F \left( \mathbf{x}_t^{\mathbf{p}_{t-1}}, \mathbf{y} \right) \tag{Struck}
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{What is the discriminant function, $F$?}
    \begin{equation}
        F(\mathbf{x}, \mathbf{y}) = \langle \mathbf{w}, \mathbf{\Phi}(\mathbf{x}, \mathbf{y}\rangle
    \end{equation}
    \begin{equation} \label{eq:minimizing_hyperplane}
        \begin{aligned}
            \min_{\mathbf{w}} \quad & \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \\
            \st & \forall i : \xi \ge 0 \\
                & \forall i, \forall \mathbf{y} \ne \mathbf{y}_i : \langle \mathbf{w}, \delta \mathbf{\Phi}_i (\mathbf{y}) \rangle \ge \Delta (\mathbf{y}_i, \mathbf{y}) - \xi_i
        \end{aligned}
    \end{equation}

    \begin{equation}
        \delta \mathbf{\Phi}_i (\mathbf{y}) = \mathbf{\Phi}(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{\Phi}(\mathbf{x}_i, \mathbf{y})
    \end{equation}
\end{frame}

\begin{frame}
    \frametitle{What is the discriminant function, $F$}
    \begin{itemize}
        \item $\Delta$ is a loss function.
            \begin{itemize}
                \item $\Delta (\mathbf{y}, \mathbf{\bar{y}}) = 0 \; \text{iff} \; \mathbf{y} = \mathbf{\bar{y}}$
                \item $\Delta$ should decrease to 0 as $\mathbf{y}$ approaches $\mathbf{\bar{y}}$
            \end{itemize}
        \item This overcomes the equal treatment of samples.
        \item The authors use a variation of bounding box overlap:
    \end{itemize}
    \begin{equation}
        \Delta(\mathbf{y}, \mathbf{\bar{y}}) = 1 - s_{\mathbf{p}_t}^o (\mathbf{y}, \mathbf{\bar{y}})
    \end{equation}
    \alert{What does this mean for $\Delta$?}
\end{frame}

\begin{frame}
    \frametitle{Optimizing equation \eqref{eq:minimizing_hyperplane}}
    \begin{equation}
        \min_{\mathbf{w}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \tag{\ref{eq:minimizing_hyperplane}}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \max_{\alpha} \quad & \sum_{i, \mathbf{y} \ne \mathbf{y}_i} \struckDelta \alpha_i^\mathbf{y} -
                \frac{1}{2} \sum_{\substack{i, \mathbf{y} \ne \mathbf{y}_i \\ j, \mathbf{y} \ne \mathbf{y}_j}}
                \alpha_i^\mathbf{y} \alpha_j^\mathbf{\bar{y}} \langle \delta \mathbf{\Phi}_i \left( \mathbf{\bar{y}} \right),
                \delta \mathbf{\Phi}_j \left( \mathbf{\bar{y}} \right) \rangle \\
            \st & \forall i, \forall \mathbf{y} \ne \mathbf{y}_i : \alpha_i^\mathbf{y} \ge 0 \\
                & \forall i \sum_{\mathbf{y} \ne \mathbf{y}_i} \alpha_i^\mathbf{y} \le C
        \end{aligned}
    \end{equation}
\end{frame}

\begin{frame}
    \frametitle{Optimizing equation \eqref{eq:minimizing_hyperplane}}
    Reparameterising based on ...
    \begin{equation}
        \beta_i^\mathbf{y} = \begin{dcases}
                                -\alpha_i^\mathbf{y} & \mathbf{y} \ne \mathbf{y}_i \\
                                \sum_{\mathbf{\bar{y}} \ne \mathbf{y}_i} \alpha_i^\mathbf{\bar{y}} & \mathbf{y} = \mathbf{y}_i
                             \end{dcases}
    \end{equation}

    \begin{equation}
        \begin{aligned}
            \max_{\beta} & -\sum_{i,\mathbf{y}} \struckDelta \beta_i^\mathbf{y} -
                \frac{1}{2} \sum_{i,\mathbf{y},j,\mathbf{\bar{y}}} \beta_i^\mathbf{y}
                \beta_j^\mathbf{\bar{y}} \langle \mathbf{\Phi}\left( \mathbf{x}_i, \mathbf{y}
                \right), \mathbf{\Phi}\left( \mathbf{x}_j, \mathbf{\bar{y}} \right) \rangle \\
            \st & \forall i, \forall \mathbf{y} : \beta_i^\mathbf{y} \le \delta(\mathbf{y}, \mathbf{y}_i) C \\
                & \forall i: \sum_\mathbf{y} \beta_i^\mathbf{y} = 0
        \end{aligned}
    \end{equation}
\end{frame}
\begin{frame}
    \frametitle{Optimizing equation \eqref{eq:minimizing_hyperplane}}
    \begin{gather}
        \delta(\mathbf{y}, \mathbf{\bar{y}}) = \begin{cases}
                                                    1 & \mathbf{y} = \mathbf{\bar{y}} \\
                                                    0 & \mathbf{y} \ne \mathbf{\bar{y}}
                                               \end{cases} \\
        F(\mathbf{x}, \mathbf{y}) = \sum_{i, \mathbf{\bar{y}}} \beta_i^\mathbf{\bar{y}} \langle
            \mathbf{\Phi}(\mathbf{x}_i, \mathbf{\bar{y}}), \mathbf{\Phi}(\mathbf{x}, \mathbf{y}) \rangle
    \end{gather}
\end{frame}

%\begin{frame}
%    \frametitle{How does Struck work?}
%    \begin{itemize}
%        \item Learning and tracking are integrated.
%        \item Uses a structured output SVM.
%        \item Must overcome the \alert{curse of kernelization.}
%            \begin{itemize}
%                \item Number of support vectors increase as a function of training data
%            \end{itemize}
%    \end{itemize}
%\end{frame}

\input{experiments}
\input{conclusion}

\end{document}

