\section{Struck}

\begin{frame}
    \frametitle{Here's the gist...}
    \begin{itemize}
        \item Don't learn a binary classifier. Learn a prediction function: \(f :
            \mathcal{X} \rightarrow \mathcal{Y}\).
        \item Labelled sample is \((\mathbf{x}, \mathbf{y})\).
        \item \(f\) is learned in a structured output SVM.
        \item Use a discriminant function \(F: \mathcal{X} \times \mathcal{Y}
            \rightarrow \mathbb{R}\).
        \item Prediction function becomes %\uncover<1>{
            \begin{align}
                \mathbf{y}_t &= \argmax\limits_{y \in \mathcal{Y}} h \left( \mathbf{x}_t^{\mathbf{p}_{t-1}}, \mathbf{y} \right) \tag{prior} \\
                \mathbf{y}_t &= \argmax\limits_{y \in \mathcal{Y}} F \left( \mathbf{x}_t^{\mathbf{p}_{t-1}}, \mathbf{y} \right) \tag{Struck}
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{What is the discriminant function, \(F\)?}
    \begin{equation}
        F(\mathbf{x}, \mathbf{y}) = \langle \mathbf{w}, \mathbf{\Phi}(\mathbf{x}, \mathbf{y})\rangle
    \end{equation}

    \begin{equation} \label{eq:minimizing_hyperplane}
        \begin{aligned}
            \min_{\mathbf{w}} \quad & \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \\
            \st & \forall i : \xi \ge 0 \\
                & \forall i, \forall \mathbf{y} \ne \mathbf{y}_i : \langle \mathbf{w}, \delta \mathbf{\Phi}_i (\mathbf{y}) \rangle \ge \Delta (\mathbf{y}_i, \mathbf{y}) - \xi_i
        \end{aligned}
    \end{equation}

    where \( \delta \mathbf{\Phi}_i (\mathbf{y}) = \mathbf{\Phi}(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{\Phi}(\mathbf{x}_i, \mathbf{y}) \)
\end{frame}

\begin{frame}
    \frametitle{What is the discriminant function, \(F\)}
    \begin{itemize}
        \item \(\Delta\) is a loss function.
            \begin{itemize}
                \item \(\Delta (\mathbf{y}, \mathbf{\bar{y}}) = 0 \; \text{iff} \; \mathbf{y} = \mathbf{\bar{y}}\)
                \item \(\Delta\) should decrease to 0 as \(\mathbf{y}\) approaches \(\mathbf{\bar{y}}\)
            \end{itemize}
        \item This overcomes the equal treatment of samples.
        \item The authors use a variation of bounding box overlap:
    \end{itemize}
    \begin{equation}
        \Delta(\mathbf{y}, \mathbf{\bar{y}}) = 1 - s_{\mathbf{p}_t}^o (\mathbf{y}, \mathbf{\bar{y}})
    \end{equation}
    \alert{What does this mean for \(\Delta\)?}
\end{frame}

\begin{frame}
    \frametitle{Optimizing equation \eqref{eq:minimizing_hyperplane}}
    \begin{equation}
        \min_{\mathbf{w}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \tag{\ref{eq:minimizing_hyperplane}}
    \end{equation}

    \begin{equation}
        \begin{aligned}
            \max_{\alpha} \quad & \sum_{i, \mathbf{y} \ne \mathbf{y}_i} \struckDelta \alpha_i^\mathbf{y} -
                \frac{1}{2} \sum_{\substack{i, \mathbf{y} \ne \mathbf{y}_i \\ j, \mathbf{y} \ne \mathbf{y}_j}}
                \alpha_i^\mathbf{y} \alpha_j^\mathbf{\bar{y}} \langle \delta \mathbf{\Phi}_i \left( \mathbf{\bar{y}} \right),
                \delta \mathbf{\Phi}_j \left( \mathbf{\bar{y}} \right) \rangle \\
            \st & \forall i, \forall \mathbf{y} \ne \mathbf{y}_i : \alpha_i^\mathbf{y} \ge 0 \\
                & \forall i \sum_{\mathbf{y} \ne \mathbf{y}_i} \alpha_i^\mathbf{y} \le C
        \end{aligned}
    \end{equation}

    via Lagrangian multipliers
\end{frame}

\begin{frame}
    \frametitle{Optimizing equation \eqref{eq:minimizing_hyperplane}}
    Reparameterising based on ...
    \begin{equation}
        \beta_i^\mathbf{y} = \begin{dcases}
                                -\alpha_i^\mathbf{y} & \mathbf{y} \ne \mathbf{y}_i \\
                                \sum_{\mathbf{\bar{y}} \ne \mathbf{y}_i} \alpha_i^\mathbf{\bar{y}} & \mathbf{y} = \mathbf{y}_i
                             \end{dcases}
    \end{equation}

    \begin{equation} \label{eq:maximizig_beta}
        \begin{aligned}
            \max_{\beta} \quad & -\sum_{i,\mathbf{y}} \struckDelta \beta_i^\mathbf{y} -
                \frac{1}{2} \sum_{i,\mathbf{y},j,\mathbf{\bar{y}}} \beta_i^\mathbf{y}
                \beta_j^\mathbf{\bar{y}} \langle \mathbf{\Phi}\left( \mathbf{x}_i, \mathbf{y}
                \right), \mathbf{\Phi}\left( \mathbf{x}_j, \mathbf{\bar{y}} \right) \rangle \\
            \st & \forall i, \forall \mathbf{y} : \beta_i^\mathbf{y} \le \delta(\mathbf{y}, \mathbf{y}_i) C \\
                & \forall i: \sum_\mathbf{y} \beta_i^\mathbf{y} = 0
        \end{aligned}
    \end{equation}
\end{frame}

\begin{frame}
    \frametitle{Optimizing equation \eqref{eq:minimizing_hyperplane}}
    \[
        \begin{aligned}
            \delta(\mathbf{y}, \mathbf{\bar{y}}) &= \begin{cases}
                                                    1 & \mathbf{y} = \mathbf{\bar{y}} \\
                                                    0 & \mathbf{y} \ne \mathbf{\bar{y}}
                                               \end{cases} \nonumber \\
            F\left(\mathbf{x}, \mathbf{y}\right) & = \sum_{i, \mathbf{\bar{y}}} \beta_i^\mathbf{\bar{y}}
                \langle \mathbf{\Phi}\left(\mathbf{x}_i, \mathbf{\bar{y}}\right),
                \mathbf{\Phi}\left(\mathbf{x}, \mathbf{y}\right) \rangle
        \end{aligned}
    \]
\end{frame}

\begin{frame}
    \frametitle{What are the consequences?}
    \begin{description}
        \item [Support Vector] \(\left(\mathbf{x}_i, \mathbf{y}\right)\) with \(\beta_i^\mathbf{y} \ne 0\).
        \item [Support Pattern] \(\mathbf{x}_i\) with at least one support vector.
    \end{description}

    For a given support pattern \(\mathbf{x}_i\):
    \begin{description}
        \item [Positive support vector] \(\beta_i^{\mathbf{y}_i} > 0\) only for \(\left(\mathbf{x}_i, \mathbf{y}_i\right)\)
        \item [Negative support vector] For all other \(\left(\mathbf{x}_i, \mathbf{y}\right)\), \(\beta_i^\mathbf{y} < 0\)
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{Sequential minimal optimization (SMO)}
    \begin{itemize}
        \item Monotonically improve \eqref{eq:maximizig_beta} with respect to
            \(\beta_i^{\mathbf{y}_+}\) and \(\beta_i^{\mathbf{y}_-}\) 
        \item Recall \( \sum_\mathbf{y} \beta_i^\mathbf{y} = 0 \)
        \item Therefore, if \( \beta_i^{\mathbf{y}_+} \) is increased by \(\lambda\),
            \( \beta_i^{\mathbf{y}_-} \) must be decremented by \(\lambda\).
        \item SMOStep algorithm updates the \(\beta\) coefficients and gradients:
    \end{itemize}
    \begin{equation}
        \lambda = \clamp_{0, C\delta\left(\mathbf{y}_+, \mathbf{y}_-\right) - \beta_i{\mathbf{y}_+}}
        \frac{g_i\left(\mathbf{y}_+\right)-g_i\left(\mathbf{y}_-\right)}
             {k\left(\mathbf{x}_i, \mathbf{y}_+\right) + k\left(\mathbf{x}_i, \mathbf{y}\right)
             -2k\left(\mathbf{x}_i, \mathbf{y}_+, \mathbf{x}_i, \mathbf{y}_-\right)}
    \end{equation}
    \begin{equation} \label{eq:gradient}
        \begin{aligned}
            g_i(\mathbf{y}) &= -\struckDelta - \sum_{j, \mathbf{\bar{y}}} \beta_j^\mathbf{\bar{y}}
            \langle \mathbf{\Phi}\left(\mathbf{x}_i, \mathbf{y}\right), \mathbf{\Phi}\left(\mathbf{x}_j,
            \mathbf{\bar{y}}\right) \rangle \\
            &= -\struckDelta - F\left(\mathbf{x}_i, \mathbf{y}\right)
        \end{aligned}
    \end{equation}
\end{frame}

%\begin{frame}
%    \frametitle{SMOStep input}
%    For \(i\), select \(\mathbf{y}_+\) and \(\mathbf{y}_-\) to define the search direction with
%    highest gradient \(g\):
%
%    \begin{equation} \label{eq:gradient}
%        \begin{aligned}
%            g_i(\mathbf{y}) &= -\struckDelta - \sum_{j, \mathbf{\bar{y}}} \beta_j^\mathbf{\bar{y}}
%            \langle \mathbf{\Phi}\left(\mathbf{x}_i, \mathbf{y}\right), \mathbf{\Phi}\left(\mathbf{x}_j,
%            \mathbf{\bar{y}}\right) \rangle \\
%            &= -\struckDelta - F\left(\mathbf{x}_i, \mathbf{y}\right)
%        \end{aligned}
%    \end{equation}
%\end{frame}

\begin{frame}
    \frametitle{Updating the learner}
    There are three update steps:
    \begin{description}
        \item [\processNew] Update with a new sample \(\left(\mathbf{x}_i, \mathbf{y}_i\right)\) \\
            Calculate \(\mathbf{y}_+ = \mathbf{y}_i\) and \(\mathbf{y}_- = \argmin_{\mathbf{y} \in
            \mathcal{Y}} g_i(\mathbf{y})\).
        \item [\processOld] Update an existing support pattern \(\mathbf{x}_i\). \\
            Calculate \(\mathbf{y}_+ = \argmax_{\mathbf{y} \in \mathcal{Y}} g_i(\mathbf{y})\). \\
            Calculate \(\mathbf{y}_- = \argmin_{\mathbf{y} \in \mathcal{Y}} g_i(\mathbf{y})\).
        \item [\optimize] Update existing support vectors for \(\mathbf{x}_i\). \\
            Calculate \(\mathbf{y}_+ = \argmax_{\mathbf{y} \in \mathcal{Y}} g_i(\mathbf{y})\). \\
            Calculate \(\mathbf{y}_- = \argmin_{\mathbf{y} \in \mathcal{Y}_i} g_i(\mathbf{y})\) for
            \(\mathcal{Y}_i = \left\{\mathbf{y} \in \mathcal{Y} | \beta_i^\mathbf{y} \ne 0
            \right\}\).
    \end{description}
    If \(\beta_i^\mathbf{y} = 0\), remove it from the SVM.
\end{frame}

\begin{frame}
    \frametitle{Budgeting for SVMs}
    \begin{itemize}
        \item \textsc{ProcessNew} and \textsc{ProcessOld} may add new support vectors.
        \item Nothing guarantees removing support vectors at the same rate. \uncover<1>{\alert{Why is this a problem?}}
        \item<2-> If left unchecked, this growth will slow the algorithm.
        \item<2-> \(F(\mathbf{x}, \mathbf{y})\) evaluates kernel functions between \((\mathbf{x},
            \mathbf{y})\) and every support vector!
        \item<2-> \processNew and \processOld both evaluate \(F\) as part of \eqref{eq:gradient}.
    \end{itemize}

    \uncover<2->{Add a budget to the SVM.}
\end{frame}

\begin{frame}
    \frametitle{How does one remove support vectors?}
    \begin{itemize}
        \item Remove the support vector which results in the smallest change \(\mathbf{w}\), evaluated as
            \(\|\Delta \mathbf{w}\|^2\).
        \item Be careful to not violate the constraint \(\sum_\mathbf{y} \beta_i^\mathbf{y} = 0\).
        \item There exists only one positive support vector for each \(\mathbf{x}_i\).
            \begin{itemize}
                \item Therefore, only negative support vectors need to be considered.
                \item When \(\mathbf{x}_i\) only has one negative support vector, both positive and
                    negative will be removed.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Removing support vectors}
    Removing negative support vector \((\mathbf{x}_r, \mathbf{y})\)...
    \begin{equation}
        \mathbf{\bar{w}} = \mathbf{w} - \beta_r^\mathbf{y} \mathbf{\Phi}\left(\mathbf{x}_r,
        \mathbf{y}\right) + \beta_r^\mathbf{y} \mathbf{\Phi}\left(\mathbf{x}_r, \mathbf{y}_r\right)
    \end{equation}
    \begin{multline}
        \|\Delta \mathbf{w}\|^2 = \left(\beta_r^\mathbf{y}\right)^2
            \langle \mathbf{\Phi}\left(\mathbf{x}_r, \mathbf{y}\right), \mathbf{\Phi}\left(\mathbf{x}_r, \mathbf{y}\right) \rangle \nonumber \\
            + \left(\beta_r^\mathbf{y}\right)^2 \langle \mathbf{\Phi}\left(\mathbf{x}_r, \mathbf{y}_r\right), \mathbf{\Phi}\left(\mathbf{x}_r, \mathbf{y}_r\right) \rangle \\
            - 2 \left(\beta_r^\mathbf{y}\right)^2 \langle \mathbf{\Phi}\left(\mathbf{x}_r, \mathbf{y}\right), \mathbf{\Phi}\left(\mathbf{x}_r, \mathbf{y}_r\right) \rangle
    \end{multline}
\end{frame}

%\begin{frame}
%    \frametitle{Optimizing equation \eqref{eq:minimizing_hyperplane}}
%    \begin{align}
%        \delta(\mathbf{y}, \mathbf{\bar{y}}) &= \begin{cases}
%                                                    1 & \mathbf{y} = \mathbf{\bar{y}} \\
%                                                    0 & \mathbf{y} \ne \mathbf{\bar{y}}
%                                               \end{cases} \nonumber \\
%        F(\mathbf{x}, \mathbf{y}) &= \sum_{i, \mathbf{\bar{y}}} \beta_i^\mathbf{\bar{y}} \langle
%            \mathbf{\Phi}(\mathbf{x}_i, \mathbf{\bar{y}}), \mathbf{\Phi}(\mathbf{x}, \mathbf{y}) \rangle
%    \end{align}
%\end{frame}

%\begin{frame}
%    \frametitle{How does Struck work?}
%    \begin{itemize}
%        \item Learning and tracking are integrated.
%        \item Uses a structured output SVM.
%        \item Must overcome the \alert{curse of kernelization.}
%            \begin{itemize}
%                \item Number of support vectors increase as a function of training data
%            \end{itemize}
%    \end{itemize}
%\end{frame}
