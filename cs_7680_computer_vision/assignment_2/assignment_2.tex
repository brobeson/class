\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{algorithm2e}

\begin{document}
\noindent Brendan Robeson

\noindent CS 7680 - Assignment 2

\noindent \today

\medskip

\begin{description}[leftmargin=0in]
    \item [Source] X. Zhao and S. Zhang, "Facial expression recognition based on
        local binary patterns and kernel discriminant isomap," \emph{Sensors,}
        vol. 11, no. 10, pp. 9573-9588, 2011. [Online]. Available:
        http://www.mdpi.com/1424-8220/11/10/9573. Cited by 47.

    \item [URL] http://www.mdpi.com/1424-8220/11/10/9573

    \item [Problem] The authors want the computer to recognize facial
        expressions in images. Facial expressions are a primary mode of
        non-verbal communication between people. Thus automating facial
        recognition has interesting applications for various fields.

    \item [Assumptions] The authors did not state any assumptions.

    \item [Data Sets] The authors used the JAFFE database
        (http://www.kasrl.org/jaffe.html) and the Cohn-Kanade database
        (http://www.pitt.edu/\~{}emotion/ck-spread.htm). The authors
        pre-processed the data to eliminate non-facial features (hair, neck,
        background, etc.).

    \item [Algorithm Overview] The authors dub their algorithm kernel
        discriminant isomap (KDIsomap). The goal of KDIsomap is to maximize
        interclass scatter, while minimizing intraclass scatter, as per LDA. The
        authors begin with kernal isomap (KIsomap). The crux of KDIsomap is to
        modify the Euclidean distance used in KIsomap.

        \begin{algorithm}
            \KwIn{LBP histograms and corresponding expression class label}
            \ForAll{$x_i$}{
                \ForAll{$x_j$}{
                    Calculate Euclidean distance with Equation (7)\;
                    Calculate kernel discriminant distance with Equation (8)\;
                    Calculate the geodesic distance\;
                }
            }
            Calculate a matrix of centered geodesic distances with Equation
            (3)\;
            Calculate a Mercer kernel matrix, $K^*$, with Equation (4)\;
            Calculate the $d$ most important eigenvectors of $K^*$\;
            Calculate the embedded coordinates with Equation (5)\;
            \KwOut{A set of feature vectors}
        \end{algorithm}

        Equation (7) is a function of the input data points $x_i$ and $x_j$. It
        describes the Euclidean distance of two data points in higher
        dimensions. This equation uses the kernel trick to avoid actually
        transforming the data.

        Equation (8) provides the kernel discriminant distance and is how the
        algorithm maintains the scatter constraints outlined above. This is a
        function of the data points' classes, and the data points' Euclidean
        distance. If the two data points have the same class, the function
        calculates a distance on $[0, 1)$. If the two data points have different
        classes, the calculated distance is on $[1, \infty)$.

        Equation (3) simply centers the geodesic distances around the mean.

        Equation (4) calculate a Mercer kernel matrix, $K^*$. This
        equation has been shown to produce a positive semi-definite matrix.
        Thus, $K^*$ satisfies Mercer's condition, and the matrix can be used
        as a kernel in lieu of transforming the data to higher dimensions.

        Finally, Equation (5) combines the eigenvalues and eigenvectors to form
        the embedded coordinates. These are the output of the algorithm; each
        column vector is a transformed data point.

    \item [Experiments] KDIsomap was compared to PCA, LDA, KPCA, kernel linear
        discriminant analysis (KLDA), and kernel isomap (KIsomap). For their
        experiments, the authors used a Gaussian kernel function; the equation
        is not numbered, but is provided in the text on page 9581.

        Quantitatively, the authors compared each algorithm's recognition
        accuracy vs. embedded dimension in a line graph. The authors also
        provide a table showing each algorithm's peak accuracy with the
        corresponding embedded dimension. In addition, a confusion matrix is
        provided for KDIsomap for seven classes of facial expressions. These
        data are provided separately for the JAFFE and Cohn-Kanade databases.
        Finally, the authors provided a table of computational and memory
        complexity for each algorithm.

        There are no qualitative figures directly related to KDIsomap.

    \item [Contributions] KDIsomap improves facial expression classification
        over KIsomap. It does so by minimizing scatter among data within a
        class, and maximizing scatter among data between classes.

    \item [Shortcomings] The authors did not mention any shortcomings in the
        paper. They did discuss integrating KDIsomap with boosted LBP and an SVM
        classifier. They also hypothesize that incorporating time information
        could lead to improved accuracy.

    \item [Self Evaluation] This is a somewhat novel approach to the problem.
        Their experiments show significant accuracy improvement over KIsomap,
        but only minor improvement over the other methods. Computational
        complexity is equal to that of KPCA and KLDA, and worse than PCA and
        LDA.

    \item [Improvements] At this time, I haven't come up with any ideas to
        improve KDIsomap.

    \item [Applications] This technique is applicable to any feature extraction
        task, provided the computational costs are acceptable.

    \item [Packages] I was unable to find any implementations of KDIsomap. The
        closest I came was a Matlab implementation of Isomap at
        https://lvdmaaten.github.io/drtoolbox/, though it is no longer being
        maintained.
\end{description}

\end{document}
