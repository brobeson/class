\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}

\begin{document}
\noindent Brendan Robeson

\noindent CS 7680 - Assignment 2

\noindent \today

\medskip

\begin{description}[leftmargin=0in]
    \item [Source] X. Zhao and S. Zhang, "Facial expression recognition based on
        local binary patterns and kernel discriminant isomap," \emph{Sensors,}
        vol. 11, no. 10, pp. 9573-9588, 2011. [Online]. Available:
        http://www.mdpi.com/1424-8220/11/10/9573. Cited by 47.

    \item [URL] http://www.mdpi.com/1424-8220/11/10/9573

    \item [Problem] The authors want the computer to recognize facial
        expressions in images. Facial expressions are a primary mode of
        non-verbal communication between people. Thus automating facial
        recognition has interesting applications for various fields.

    \item [Assumptions] The authors did not state any assumptions.

    \item [Data Sets] The authors used the JAFFE database
        (http://www.kasrl.org/jaffe.html) and the Cohn-Kanade database
        (http://www.pitt.edu/\~{}emotion/ck-spread.htm). The authors
        pre-processed the data to eliminate non-facial features (hair, neck,
        background, etc.).

    \item [Algorithm Overview] The authors dub their algorithm kernel
        discriminant isomap (KDIsomap). The goal of KDIsomap is to maximize
        interclass scatter, while minimizing intraclass scatter, as per LDA. The
        authors begin with kernal isomap (KIsomap). The crux of KDIsomap is to
        modify the Euclidean distance used in KIsomap.

        Input for the algorithm is the image data and the corresponding class
        label. The image data point is a vector of length D. The class label is
        one of the facial expressions of interest.

        The first step is to implicitly map the input data to a higher dimensional
        space. A nonlinear function, \(\phi\), performs this mapping. A kernel
        function is used to avoid actually performing the math in higher
        dimensions, as shown with equation (6). For their experiments, the
        authors used a Gaussian kernel function; the equation is not numbered,
        but is provided in the text on page 9581.

        Second, a neighborhood graph is constructed for the high dimensional
        data. The data points form the graph vertices. The edges connect a point
        to its nearest neighbors, and are weighted with the kernel discriminant
        distance between two vertices. For this step, the Euclidean distance as
        given by Equation (7), is required. This is where the kernel trick is
        used, to calculate the Euclidean distance in higher dimensions, without
        actually transforming the data. Equation (8) provides the kernel
        discriminant distance and is how the algorithm maintains the scatter
        constraints outlined above. Equation (8) is a function of the data
        points' classes, and the data points' Euclidean distance. If the two
        data points have the same class, the function calculates a distance on
        \([0, 1)\). If the two data points have different classes, the
        calculated distance is on \([1, \infty)\).

        The remaining steps are taken from KIsomap. The third step of KDIsomap
        is to calculate a matrix of geodesic distances between all the data
        points. The authors suggest using Dijkstra's algorithm. Each matrix
        element is simply the square of the geodesic distance between two data
        points.

        The fourth step is to calculate the matrix kernel \(K(D^2)\) using
        equation (3). The kernel matrix is a centering of the geodesic distance
        matrix from the previous step.

        Next, calculate a Mercer kernel matrix \(K^*\) from equation (4). This
        equation has been shown to produce a positive semi-definite matrix.
        Thus, \(K^*\) satisifies Mercer's condition, and the matrix can be used
        as a kernel in lieu of transforming the data to higher dimensions. From
        \(K^*\) calculate the \emph{d} most important eigenvectors and their
        corresponding eigenvalues.

        The final step is to reduce the input data from D dimensions to d
        dimensions. This transformation is given by equation (5). The output is
        a \(d \times N\) matrix; each column vector is a transformed data point.

    \item [Experiments] KDIsomap was compared to PCA, LDA, KPCA, kernel linear
        discriminant analysis (KLDA), and kernel isomap (KIsomap).

        Quantitatively, the authors compared each algorithm's recognition
        accuracy vs. embedded dimension in a line graph. The authors also
        provide a table showing each algorithm's peak accuracy with the
        corresponding embedded dimension. In addition, a confusion matrix is
        provided for KDIsomap for seven classes of facial expressions. These
        data are provided separately for the JAFFE and Cohn-Kanade databases.
        Finally, the authors provided a table of computational and memory
        complexity for each algorithm.

        There are no qualitative figures directly related to KDIsomap.

    \item [Contributions] KDIsomap improves facial expression classification
        over KIsomap. It does so by minimizing scatter among data within a
        class, and maximizing scatter among data between classes.

    \item [Shortcomings] The authors did not mention any shortcomings in the
        paper. They did discuss integrating KDIsomap with boosted LBP and an SVM
        classifier. They also hypothesize that incorporating time information
        could lead to improved accuracy.

    \item [Self Evaluation] This is a somewhat novel approach to the problem.
        Their experiments show significant accuracy improvement over KIsomap,
        but only minor improvement over the other methods. Computational
        complexity is equal to that of KPCA and KLDA, and worse than PCA and
        LDA.

    \item [Improvements] At this time, I haven't come up with any ideas to
        improve KDIsomap.

    \item [Applications] This technique is applicable to any feature extraction
        task, provided the computational costs are acceptable.

    \item [Packages] I was unable to find any implementations of KDIsomap. The
        closest I came was a Matlab implementation of Isomap at
        https://lvdmaaten.github.io/drtoolbox/, though it is no longer being
        maintained.
\end{description}

\end{document}
