This is a review of “Detecting Events in Streaming Multimedia with Big Data Techniques” [1]. This article was retrieved on 2016 June 29 from https://ieeexplore.org.dist.lib.usu.edu/xpl/articleDetails.jsp?arnumber=74453354. There does not appear to be any source code available written by the authors. The authors did, however, use some open-source packages: Hadoop (hadoop.apache.org), R (www.r-project.org), CMU Sphinx (cmusphinx.sourceforge.net), and LIUM Speaker Diarization (http://www-lium.univ-lemans.fr/diarization/doku.php/welcome).

The authors are attempting to extract information from internet streaming video and audio. Specifically, the authors want to determine how often a particular term is used by multiple feeds, and which terms are used most often in a specific feed [1]. Practical uses for such technology range from recommendation services, to law enforcement services. A recommendation service could analyze podcasts a user listens to for the most common terms, then recommend other podcasts which frequently use the same terms. Law enforcement could employ this technology to monitor video and audio broadcasts from suspected criminals and terrorists. Their system could monitor the suspects’ feeds for terms related to the suspected crime.

The authors do not explicitly describe any assumptions. However, there are some potential pitfalls that they do not mention either. For example, what if the audio becomes unintelligible at the source? The authors do not describe any constraints on the video resolution. Though their paper mainly dealt with audio, capturing 4K video will increase total processing time over lower resolution video. Source audio quality will have a similar impact, though to a lesser extent.

The authors do not provide their exact dataset. They do explain that they acquired several audio files from www.voxforge.org/es, which they combined into a file consisting of one hour of audio. They do not describe which audio from VoxForge was used.

This paper presents not so much a new algorithm, as a method of utilizing existing technologies to solve the problem. The authors present six broad steps: capturing source broadcasts, normalizing the captured data, speaker diarization, speech recognition, extracting information, and displaying the results. The process presented eliminated the first two steps from consideration by combining several audio files into a normalized file. This allowed the authors to focus solely on extracting information from the audio. Speaker diarization splits the audio stream into separate streams, one for each speaker. This is done to improve the efficiency of later steps; the speakers’ streams can be processed in parallel. The speech recognition step converts each speaker’s audio to text. This feeds directly to extracting information by word searching of the text output. Finally, charts of the results are generated for the user.

The authors did not compare their results with other methods of solving this problem. The comparisons presented in the paper compare executions of their process with different Hadoop configurations. Hadoop allows the user to configure the file block size, and this setting affects the efficiency of the tasks performed on the cluster. The authors used 32, 64, 128, and 256 MB block sizes for their tests. Most tables in the paper present execution times for each of the four block sizes.

In the future, the authors intend to compare different distributed processing frameworks; the explicitly mention Apache Storm and Apache Spark. They also plan to confirm that the process works for video streams.

The ideas presented in the paper are interested and produce promising results. However, the process is basically a combination of existing technologies, not a new technology in and of itself. I don’t see anything to be done algorithmically to improve the process. Of course, more powerful hardware could be employed for processing denser data, such as 4K video. I addition, I think some of the software employed is written in Java. I’m curious how much difference a compiled language, such as C or C++, would make over an interpreted language.

The crux of extracting information from an audio stream is converting the audio to text, then searching the text. Thus, this approach is applicable to anything which can be converted to text. Any aspect of image or video processing which can be described textually, could have the results searched in a similar manner: recognized targets, the direction of tracked targets, etc.

1. J. Herrera and G. Molto, "Detecting Events in Streaming Multimedia with Big Data Techniques," *2016 24th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP)*, Heraklion, 2016, pp. 345-349. doi: 10.1109/PDP.2016.45
